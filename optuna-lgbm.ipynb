{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Welcome to the third tabular playground of 2021"},{"metadata":{},"cell_type":"markdown","source":"In this notebook I did a fast exploratory data analysis. Then, I tuned lgbm hyperparameters using Optuna. Finally, I use a small trick that enables me to obtain a 15th place in last tabular playground."},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\n\nfrom sklearn.metrics import roc_auc_score\nimport optuna\n\nplt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('../input/tabular-playground-series-mar-2021/train.csv')\ntest=pd.read_csv('../input/tabular-playground-series-mar-2021/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Number of rows in training set: {train.shape[0]}')\nprint(f'Number of features in training set: {train.shape[1]}')\nprint(f'Number of rows in test set: {test.shape[0]}')\nprint(f'Number of features in test set: {test.shape[1]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are more columns than in the previous tabular playground."},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the number of categorical and continuous variables\ntrain.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# separating categorical columns from continuous ones\ncat_var=[f'cat{i}' for i in range(19)]\ncont_var=[f'cont{i}' for i in range(11)]\ncolumns=cat_var+cont_var","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"cat7, cat8 and cat10 have very high cardinality. We will need to explore thme later."},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simple preprocessing using OnehotEncoder\n\nfull=pd.concat([train,test],axis=0)\n\nfull=pd.get_dummies(full,columns=cat_var)\n\n#for cat in cat_var:\n#    le=LabelEncoder()\n#    full[cat]=le.fit_transform(full[cat])\n    \ntrain=full.iloc[:len(train),:]\ntest=full.iloc[len(train):,:]\n\ncolumns=[column for column in train.columns if column not in ['id','target']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=train[columns]\ny=train.target\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LGBM Baseline"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb=LGBMClassifier()\nlgb.fit(X_train,y_train,eval_set=(X_test,y_test),early_stopping_rounds=200, verbose=False)\npredictions=lgb.predict_proba(X_test)[:,1]\n\nauc=roc_auc_score(y_test,predictions)\n\nprint(f'Baseline Score: {auc}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter tuning using Optuna"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial,X=X,y=y):\n    \n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n    \n    \n    lgb_params={\n        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2),\n        'max_depth': trial.suggest_int('max_depth', 6, 127),\n        'num_leaves': trial.suggest_int('num_leaves', 31, 128),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0),\n        'random_state': 2021,\n        'metric': 'auc',\n        'n_estimators': 20000,\n        'n_jobs': -1,\n        'cat_feature': [x for x in range(len(cat_var))],\n        'bagging_seed': 2021,\n        'feature_fraction_seed': 2021,\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 0.9),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n        'subsample_freq': trial.suggest_int('subsample_freq', 1, 10),\n        'subsample': trial.suggest_float('subsample', 0.3, 0.9),\n        'max_bin': trial.suggest_int('max_bin', 128, 1024),\n        'min_data_per_group': trial.suggest_int('min_data_per_group', 50, 200),\n        'cat_smooth': trial.suggest_int('cat_smooth', 10, 100),\n        'cat_l2': trial.suggest_int('cat_l2', 1, 20)}\n   \n    lgb=LGBMClassifier(**lgb_params)\n    lgb.fit(X_train,y_train,eval_set=(X_test,y_test),eval_metric='auc',early_stopping_rounds=100,verbose=False)\n    predictions=lgb.predict_proba(X_test)[:,1]\n        \n    return roc_auc_score(y_test,predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study = optuna.create_study(direction='maximize') \nstudy.optimize(objective, timeout=3600*7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study.best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_params={'learning_rate': 0.00605886703283976,\n 'max_depth': 42,\n 'num_leaves': 108,\n 'reg_alpha': 0.9140720355379223,\n 'reg_lambda': 9.97396811596188,\n 'colsample_bytree': 0.2629101393563821,\n 'min_child_samples': 61,\n 'subsample_freq': 2,\n 'subsample': 0.8329687190743886,\n 'max_bin': 899,\n 'min_data_per_group': 73,\n 'cat_smooth': 21,\n 'cat_l2': 11,\n            'random_state': 2021,\n            'metric': 'auc',\n            'n_estimators': 20000,\n            'n_jobs': -1,\n            'bagging_seed': 2021,\n            'feature_fraction_seed': 2021\n           }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extreme tuning strategy"},{"metadata":{"trusted":true},"cell_type":"code","source":"f1= 0.7434828307047571 \nf2= 1.3786330168495677\nf3= 46\nf4= 27","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nkf=StratifiedKFold(n_splits=5,random_state=48,shuffle=True)\n\n# we will store our final predictions in preds\npreds = np.zeros(test.shape[0])\n#store rmse of each iterations\nauc=[]\ni=0\n\n# --------------------------------------------------------------------------------\n# Phase 1: create the pretrained model\nfor idx_train,idx_test in kf.split(X,y):\n    \n    X_train,X_test=X.iloc[idx_train],X.iloc[idx_test]\n    y_train,y_test=y.iloc[idx_train],y.iloc[idx_test]\n\n    \n    model=LGBMClassifier(**lgb_params)\n    \n    model.fit(X_train,y_train,eval_set=(X_test,y_test),early_stopping_rounds=300,verbose=False,eval_metric='auc')\n    \n    predictions=model.predict_proba(X_test,num_iteration=model.best_iteration_)[:,1]\n    \n    auc.append(roc_auc_score(y_test,predictions))\n    \n    print('First Round:')\n    \n    print(f'RMSE {auc[i]}')\n    \n    auc_tuned=[]\n    params = lgb_params.copy()\n    \n    # -----------------------------------------------------------------------------\n    # Phase 2: iterations where we decrease the learning rate and regularization params    \n    for t in range(1,18):\n        \n        \n        if t >1:    \n                    \n            params['reg_lambda'] *=  f1\n            params['reg_alpha'] += f2\n            params['num_leaves'] += f3\n            params['min_child_samples'] -= f4\n        \n        if params['min_child_samples']<1:\n            params['min_child_samples']=1\n            \n           \n        params['learning_rate']=0.003\n        \n              \n        model=LGBMClassifier(**params).fit(X_train,y_train,eval_set=(X_test,y_test),eval_metric='auc',early_stopping_rounds=200,verbose=False,init_model=model)\n        \n        predictions=model.predict_proba(X_test, num_iteration= model.best_iteration_)[:,1]\n        \n        auc_tuned.append(roc_auc_score(y_test,predictions))\n        \n        print(f'RMSE tuned {t}: {auc_tuned[t-1]}')\n        \n    print(f'Improvement of {auc_tuned[t-1]-auc[i]}')\n    \n    # ---------------------------------------------------------------------------\n    # Inference time: calculate predictions for test set\n    \n    preds+=model.predict_proba(test[columns],num_iteration=model.best_iteration_)[:,1]/kf.get_n_splits()\n        \n    i+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"%%time\n\nkf=StratifiedKFold(n_splits=5,random_state=48,shuffle=True)\n\n# we will store our final predictions in preds\npreds = np.zeros(test.shape[0])\n#store rmse of each iterations\nauc=[]\ni=0\n\n# --------------------------------------------------------------------------------\n# Phase 1: create the pretrained model\nfor idx_train,idx_test in kf.split(X,y):\n    \n    X_train,X_test=X.iloc[idx_train],X.iloc[idx_test]\n    y_train,y_test=y.iloc[idx_train],y.iloc[idx_test]\n\n    \n    model=LGBMClassifier(**lgb_params)\n    \n    model.fit(X_train,y_train,eval_set=(X_test,y_test),early_stopping_rounds=300,verbose=False,eval_metric='auc')\n    \n    predictions=model.predict_proba(X_test,num_iteration=model.best_iteration_)[:,1]\n    \n    auc.append(roc_auc_score(y_test,predictions))\n    \n    print('First Round:')\n    \n    print(f'RMSE {auc[i]}')\n    \n    auc_tuned=[]\n    params = lgb_params.copy()\n    \n    # -----------------------------------------------------------------------------\n    # Phase 2: iterations where we decrease the learning rate and regularization params    \n    for t in range(1,18):\n        \n        \n        if t >1:    \n                    \n            params['reg_lambda'] *=  f1\n            params['reg_alpha'] += f2\n            params['num_leaves'] += f3\n            params['min_child_samples'] -= f4\n        \n        if params['min_child_samples']<1:\n            params['min_child_samples']=1\n            \n           \n        params['learning_rate']=0.003\n        \n              \n        model=LGBMClassifier(**params).fit(X_train,y_train,eval_set=(X_test,y_test),eval_metric='auc',early_stopping_rounds=200,verbose=False,init_model=model)\n        \n        predictions=model.predict_proba(X_test, num_iteration= model.best_iteration_)[:,1]\n        \n        auc_tuned.append(roc_auc_score(y_test,predictions))\n        \n        print(f'RMSE tuned {t}: {auc_tuned[t-1]}')\n        \n    print(f'Improvement of {auc_tuned[t-1]-auc[i]}')\n    \n    # ---------------------------------------------------------------------------\n    # Inference time: calculate predictions for test set\n    \n    preds+=model.predict_proba(test[columns],num_iteration=model.best_iteration_)[:,1]/kf.get_n_splits()\n        \n    i+=1"},{"metadata":{},"cell_type":"markdown","source":"# Making final submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create submission file\ntest['target']=preds\ntest=test[['id','target']]\ntest.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Thanks for reading"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}